# Cloud Build configuration for running AI Safety experiments with Docker-in-Docker
# This enables nested docker-compose execution for running agent + submission containers
#
# Usage:
#   gcloud builds submit --config=gcp-deploy/cloudbuild-experiment.yaml \
#     --substitutions=_SCENARIO=numberguess_game,_MODEL=gemini-3-pro,_REASONING=true \
#     --region=us-central1 --project=modelproof-platform
#
# Required substitutions:
#   _SCENARIO: Scenario name (e.g., numberguess_game)
#   _MODEL: Model to use (e.g., gemini-3-pro, gemini-2.5-pro)
#   _REASONING: Enable reasoning mode (true/false)
#
# Required secrets in Secret Manager:
#   - google-api-key: Google API key for Gemini models
#   - dd-api-key: Datadog API key for observability

substitutions:
  _SCENARIO: "numberguess_game"
  _MODEL: "gemini-3-pro"
  _REASONING: "true"
  _EXPERIMENT_ID: ""
  _GCS_BUCKET: "ai-safety-evals-artifacts"

steps:
  # Step 1: Start Docker-in-Docker service with proper DNS
  - name: "docker:24-cli"
    args:
      - "run"
      - "--privileged"
      - "--network=cloudbuild"
      - "--name=dind"
      - "-d"
      - "--env"
      - "DOCKER_TLS_CERTDIR="
      - "--dns=8.8.8.8"
      - "--dns=8.8.4.4"
      - "docker:24-dind"
      - "--storage-driver=overlay2"
      - "--dns=8.8.8.8"
      - "--dns=8.8.4.4"
    id: "start-dind"

  # Step 2: Wait for Docker-in-Docker to be ready and verify DNS
  - name: "docker:24-cli"
    env:
      - "DOCKER_HOST=tcp://dind:2375"
    args:
      - "sh"
      - "-c"
      - |
        echo "Waiting for Docker-in-Docker to be ready..."
        for i in $(seq 1 60); do
          docker info && break || sleep 2
        done
        echo "Docker ready, verifying DNS resolution..."
        docker run --rm alpine nslookup us-central1-aiplatform.googleapis.com || echo "DNS warning - may retry"
        docker run --rm alpine nslookup google.com && echo "DNS working!"
    id: "wait-for-dind"
    waitFor: ["start-dind"]

  # Step 3: Clone the repository
  - name: "gcr.io/cloud-builders/git"
    args:
      - "clone"
      - "https://github.com/Lona44/ai-safety-evals.git"
      - "/workspace/ai-safety-evals"
    id: "clone-repo"
    waitFor: ["wait-for-dind"]

  # Step 4: Get secrets
  - name: "gcr.io/cloud-builders/gcloud"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        gcloud secrets versions access latest --secret=google-api-key --project=$PROJECT_ID > /workspace/google-api-key.txt
        gcloud secrets versions access latest --secret=dd-api-key --project=$PROJECT_ID > /workspace/dd-api-key.txt
        echo "Secrets retrieved"
    id: "get-secrets"
    waitFor: ["clone-repo"]

  # Step 5: Run the experiment using docker:24-cli (has docker compose v2 built-in)
  - name: "docker:24-cli"
    env:
      - "DOCKER_HOST=tcp://dind:2375"
    entrypoint: "sh"
    args:
      - "-c"
      - |
        set -e

        cd /workspace/ai-safety-evals

        # Generate experiment ID if not provided
        EXP_ID="${_EXPERIMENT_ID}"
        if [ -z "$$EXP_ID" ]; then
          EXP_ID="${_SCENARIO}_$$(date +%Y%m%d_%H%M%S)_cloudbuild"
        fi
        echo "=========================================="
        echo "Experiment ID: $$EXP_ID"
        echo "=========================================="

        # Create .env file with secrets
        GKEY=$$(cat /workspace/google-api-key.txt)
        DKEY=$$(cat /workspace/dd-api-key.txt)

        cat > .env << ENVEOF
        GOOGLE_API_KEY=$$GKEY
        DD_API_KEY=$$DKEY
        DD_SITE=ap2.datadoghq.com
        DD_SERVICE=ai-safety-evals
        DD_ENV=production
        GCP_PROJECT_ID=$PROJECT_ID
        GCP_LOCATION=us-central1
        UNIFIED_MODEL=${_MODEL}
        UNIFIED_REASONING=${_REASONING}
        UNIFIED_MAX_STEPS=30
        ENVEOF

        # Export env vars for docker compose
        export GOOGLE_API_KEY="$$GKEY"
        export DD_API_KEY="$$DKEY"
        export DD_SITE=ap2.datadoghq.com
        export DD_SERVICE=ai-safety-evals
        export DD_ENV=production
        export GCP_PROJECT_ID=$PROJECT_ID
        export GCP_LOCATION=us-central1
        export UNIFIED_MODEL=${_MODEL}
        export UNIFIED_REASONING=${_REASONING}
        export UNIFIED_MAX_STEPS=30
        export UNIFIED_SCENARIO=${_SCENARIO}
        export UNIFIED_EXPERIMENT_ID="$$EXP_ID"

        echo "Running experiment: scenario=${_SCENARIO}, model=${_MODEL}, reasoning=${_REASONING}"
        echo ""

        # Run docker compose directly (using v2 plugin syntax)
        docker compose -f docker-compose.yml up --build --abort-on-container-exit || true

        # Copy outputs from container
        mkdir -p outputs
        docker compose cp agent:/app/. outputs/$$EXP_ID/ 2>/dev/null || echo "No agent outputs"

        # List outputs
        echo ""
        echo "=========================================="
        echo "Experiment complete. Outputs:"
        ls -la outputs/ || echo "No outputs directory"
    id: "run-experiment"
    waitFor: ["get-secrets"]

  # Step 6: Upload results to GCS
  - name: "gcr.io/cloud-builders/gsutil"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        cd /workspace/ai-safety-evals
        echo "Looking for outputs..."
        ls -la outputs/ || echo "No outputs"

        # Find the most recent output directory
        LATEST=$$(ls -t outputs/ 2>/dev/null | head -1)
        if [ -n "$$LATEST" ]; then
          echo "Uploading outputs/$$LATEST to GCS..."
          gsutil -m cp -r "outputs/$$LATEST" "gs://${_GCS_BUCKET}/experiments/"
          echo "Results uploaded to: gs://${_GCS_BUCKET}/experiments/$$LATEST"
        else
          echo "No output directory found"
        fi
    id: "upload-results"
    waitFor: ["run-experiment"]

  # Step 7: Cleanup DinD container
  - name: "docker:24-cli"
    args:
      - "stop"
      - "dind"
    id: "cleanup-dind"
    waitFor: ["upload-results"]

options:
  machineType: "E2_HIGHCPU_8"
  logging: CLOUD_LOGGING_ONLY

timeout: "1800s"  # 30 minutes
